# ML работает с данными
# - средство для создания моделей данных

# Решает следующую задачу:
# Требуется подогнать заданный набор точек с данными под некоторую функцию
# (отображение входа на выход), которая улавливает важные сигналы в данных и
# игнорирует помехи, а затем убеждается, что на новых данных найденная функция работает хорошо

# Данные -> Обучение -> Модель
# Процедура проверки модели - предсказание
# Источник (природа) данных важна!

# Виды МО:
# - обучение с учителем (supervised learning)
# - обучение без учителя (unsupervised learning)

# Обучение с уч. моделирует отношение между признаками и метками
# такие модели служат для предсказания меток на основе обучающих данных (маркированных)
# После построения модели можно использовать её для присвоения меток новым данным

# - задачи классификации (метки - дискретные)
# - задачи регрессии (метки/результат - непрерывные величины)


# Обучение без уч. - моделирование признаков без меток
# Такие модели служат для выявления структуры немаркированных данных

# - задачи кластеризации (выделяет отдельные группы данных)
# - задачи понижения размерности (поиск более сжатого представления данных)

# Существуют методы частичного обучения (semi - supervised learning)
# Метки неполные (не все данные промаркированы)

# Методы обучения с подкреплением (reinforcement learning)
# Система обучения улучшает свои характеристики на основе взаимодействия со средой
# При этом взаимодействии система получает сигналы (функции наград), которые несут в себе
# информацию насколько хорошо/ плохо система решила задачу с точки зрения среды.
# Пока итоговая награда не станет максимальной

# import seaborn as sns
#
# iris = sns.load_dataset("iris")
#
# print(iris.head())
# print(type(iris))
#
# print(type(iris.values))
# print(iris.values.shape)
# print(iris.columns)
# print(iris.index)

# Строки - отдельные объекты - образцы (sample)
# Столбцы - признаки (features) - соответствуют конкретным наблюдениям
# Матрицы признаков (features matrix), размер [число образцов][число признаков]
# Целевой массив, массив меток (targets) - одномерный массив [][число образцов]
# - данные, которые мы хотим предсказать на основе имеющихся данных
# Зависимые (метка) и независимые переменные (признаки)

# Процесс построения системы машинного обучения:
# 1. Предварительная обработка
# - На вход поступают необработанные данные и метки
# - Происходит выбор признаков, масштабирование признаков
# - (возможно) Понижение размерности
# - Выборка образцов
# - На выход поступает набор данных: обучающий и тестовый
# 2. Обучение модели
# - выбор модели
# - перекрёстная проверка
# - метрики эффективности модели
# - оптимизация гиперпараметров - параметры, которые получаются не из данных, а являются настраиваемыми характеристиками
# 3. Оценка и формирование финальной модели
# 4. Прогнозирование (применение модели)


# SciKit-learn
# 1. Выбираем класс модели
# 2. Устанавливаем гиперпараметры модели
# 3. Создаём матрицу признаков и целевой массив
# 4. Обучение модели fit()
# 5. Применить модель к новым данным
# - predict() (с учителем)
# - predict() / transform() (без учителя)

# Обучение с учителем. Линейная регрессия

# Простая линейная регрессия
# y = ax + b, y - метка, x - признак

import matplotlib.pyplot as plt
import numpy as np

# I
np.random.seed(1)
x = 10 * np.random.rand(50)

y = 2 * x + np.random.randn(50)

plt.scatter(x, y)

# II
# 1. Выбираем класс модели
from sklearn.linear_model import LinearRegression

# 2. Устанавливаем гиперпараметры модели
model = LinearRegression()
# model = LinearRegression(fit_intercept=False)

# 3. Создаём матрицу признаков и целевой массив
print(x.shape)
print(y.shape)
# добавим второе измерение, так как fit это требует
X = x[:, np.newaxis]

# 4. Обучение модели fit()
model.fit(X, y)
print(model.coef_[0])  # a
print(model.intercept_)  # b

x_ = np.linspace(0, 10, 30)
y_ = model.coef_[0] * x_ + model.intercept_

plt.plot(x_, y_)

# 5. Применить модель к новым данным

xfit = np.linspace(0, 10, 5)
yfit = model.predict(xfit[:, np.newaxis])

plt.scatter(xfit, yfit)

plt.show()
